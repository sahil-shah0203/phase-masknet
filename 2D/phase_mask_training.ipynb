{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e159445",
   "metadata": {},
   "source": [
    "\n",
    "# Phase Mask Generation Model (PhaseMaskNet)\n",
    "\n",
    "## Purpose\n",
    "Train a model (PhaseMaskNet) to predict phase masks that can reconstruct cross-sectional images at specified z-heights for holographic projection.\n",
    "\n",
    "This project aims to eventually integrate metasurfaces into additive manufacturing workflows by projecting complex layers during the build process.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b19e77",
   "metadata": {},
   "source": [
    "\n",
    "## Setup\n",
    "- Install necessary packages (`piq`, `torch`, `PIL`, etc.)\n",
    "- Mount Google Drive if using external datasets.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install piq\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import piq\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7066c3b",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preparation\n",
    "\n",
    "- Loads cross-section images from a folder.\n",
    "- Assumes images are in `.png` format, grayscale.\n",
    "- Resizes all images to a fixed size (currently low-resolution: 128×128 — **recommended to increase later**).\n",
    "- Applies optional data augmentation if specified.\n",
    "\n",
    "The dataset expects a folder structure like:\n",
    "\n",
    "```\n",
    "/path/to/cross_sections/\n",
    "    slice_001.png\n",
    "    slice_002.png\n",
    "    ...\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b026b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CrossSectionDataset(Dataset):\n",
    "    def __init__(self, img_dir, size=128, augment=True):\n",
    "        self.img_dir = img_dir\n",
    "        self.size = size\n",
    "        self.augment = augment\n",
    "        self.img_files = sorted([\n",
    "            f for f in os.listdir(img_dir)\n",
    "            if f.lower().endswith('.png')\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img = img.resize((self.size, self.size))\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "\n",
    "        if self.augment and np.random.rand() > 0.5:\n",
    "            img = np.fliplr(img)\n",
    "\n",
    "        img = torch.from_numpy(img).unsqueeze(0)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586a0afc",
   "metadata": {},
   "source": [
    "\n",
    "## Model Architecture - PhaseMaskNet\n",
    "\n",
    "- Model is based on a U-Net style encoder-decoder.\n",
    "- Designed to predict phase masks instead of regular images.\n",
    "- **This architecture is the core result of our project.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class PhaseMaskNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, features=[32, 64, 128]):\n",
    "        super(PhaseMaskNet, self).__init__()\n",
    "        self.encoder1 = DoubleConv(in_channels, features[0])\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.encoder2 = DoubleConv(features[0], features[1])\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.bottleneck = DoubleConv(features[1], features[2])\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(features[2], features[1], kernel_size=2, stride=2)\n",
    "        self.decoder2 = DoubleConv(features[2], features[1])\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(features[1], features[0], kernel_size=2, stride=2)\n",
    "        self.decoder1 = DoubleConv(features[1], features[0])\n",
    "\n",
    "        self.conv_final = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool2(enc2))\n",
    "\n",
    "        dec2 = self.upconv2(bottleneck)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        return self.conv_final(dec1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc90630",
   "metadata": {},
   "source": [
    "\n",
    "## Loss Function - SSIM\n",
    "\n",
    "- SSIM focuses on perceptual similarity rather than pixel-wise differences.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0cc3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ssim_loss(predicted, target):\n",
    "    return 1 - piq.ssim(predicted, target, data_range=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae3229",
   "metadata": {},
   "source": [
    "\n",
    "## Training Setup\n",
    "\n",
    "- Simple PyTorch training loop.\n",
    "- Train until loss convergence.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe095ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 4\n",
    "num_epochs = 30\n",
    "\n",
    "dataset = CrossSectionDataset(img_dir='/content/drive/MyDrive/slices', size=128)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = PhaseMaskNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = ssim_loss(outputs, inputs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"phase_mask_net.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02498e99",
   "metadata": {},
   "source": [
    "\n",
    "## Results and Visualization\n",
    "\n",
    "- Compare input slice vs predicted phase mask.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d8ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "n_samples = 4\n",
    "fig, axs = plt.subplots(n_samples, 2, figsize=(6, n_samples * 3))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    img = dataset[i].unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "\n",
    "    input_img = img.squeeze().cpu().numpy()\n",
    "    output_img = output.squeeze().cpu().numpy()\n",
    "\n",
    "    axs[i, 0].imshow(input_img, cmap='gray')\n",
    "    axs[i, 0].set_title(\"Input Slice\")\n",
    "    axs[i, 0].axis('off')\n",
    "\n",
    "    axs[i, 1].imshow(output_img, cmap='gray')\n",
    "    axs[i, 1].set_title(\"Predicted Phase Mask\")\n",
    "    axs[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da0b0e",
   "metadata": {},
   "source": [
    "\n",
    "## Future Work and Next Steps\n",
    "\n",
    "1. **Upgrade resolution**: move to 512×512 or higher.\n",
    "2. **Use real-world STL slices**: for better realism.\n",
    "3. **Deploy model**: inference notebook provided.\n",
    "4. **Explore full 3D models**.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
